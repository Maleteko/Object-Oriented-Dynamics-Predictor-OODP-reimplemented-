@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:home/malte/Documents/Uni/Semester4/NN/Seminar/NNSeminar{\_}OODP/docs/paper/MnihEtAlHassibis15NatureControlDeepRL.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
volume = {518},
year = {2015}
}
@article{Carlos2010,
author = {Carlos, B Y and Diuk, Gregorio},
file = {:home/malte/Documents/Uni/Semester4/NN/Seminar/NNSeminar{\_}OODP/docs/paper/OORL.pdf:pdf},
journal = {Representations},
title = {{AN OBJECT-ORIENTED REPRESENTATION FOR EFFICIENT REINFORCEMENT LEARNING Table of Contents}},
year = {2010}
}
@article{Woof2018,
abstract = {Deep reinforcement learning (DRL) has proven to be an effective tool for creating general video-game AI. However most current DRL video-game agents learn end-to-end from the video-output of the game, which is superfluous for many applications and creates a number of additional problems. More importantly, directly working on pixel-based raw video data is substantially distinct from what a human player does. In this paper, we present a novel method which enables DRL agents to learn directly from object information. This is obtained via use of an object embedding network (OEN) that compresses a set of object feature vectors of different lengths into a single fixed-length unified feature vector representing the current game-state and fulfills the DRL simultaneously. We evaluate our OEN-based DRL agent by comparing to several state-of-the-art approaches on a selection of games from the GVG-AI Competition. Experimental results suggest that our object-based DRL agent yields performance comparable to that of those approaches used in our comparative study.},
archivePrefix = {arXiv},
arxivId = {1803.05262},
author = {Woof, William and Chen, Ke},
doi = {10.1109/CIG.2018.8490438},
eprint = {1803.05262},
file = {:home/malte/Documents/Uni/Semester4/NN/Seminar/NNSeminar{\_}OODP/docs/paper/Object{\_}Embedding{\_}Network.pdf:pdf},
isbn = {9781538643594},
issn = {23254289},
journal = {IEEE Conference on Computatonal Intelligence and Games, CIG},
keywords = {Artificial neural networks,Computer games,Deep Q-learning,General video game AI,Reinforcement learning},
pages = {1--8},
title = {{Learning to Play General Video-Games via an Object Embedding Network}},
volume = {2018-August},
year = {2018}
}
@article{Silver2016,
abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, su-perhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated posi-tions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here, we introduce an algorithm based solely on reinforcement learning, without hu-man data, guidance, or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of tree search, re-sulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. Much progress towards artificial intelligence has been made using supervised learning sys-tems that are trained to replicate the decisions of human experts 1–4 . However, expert data is often expensive, unreliable, or simply unavailable. Even when reliable data is available it may impose a ceiling on the performance of systems trained in this manner 5 . In contrast, reinforcement learn-ing systems are trained from their own experience, in principle allowing them to exceed human capabilities, and to operate in domains where human expertise is lacking. Recently, there has been rapid progress towards this goal, using deep neural networks trained by reinforcement learning. These systems have outperformed humans in computer games such as Atari 6, 7 and 3D virtual en-vironments 8–10 . However, the most challenging domains in terms of human intellect – such as the 1},
author = {Silver, D and Schrittwieser, J and Simonyan, K and Nature, I Antonoglou - and 2017, Undefined},
file = {:home/malte/Documents/Uni/Semester4/NN/Seminar/NNSeminar{\_}OODP/docs/paper/agz{\_}unformatted{\_}nature.pdf:pdf},
journal = {Nature},
number = {7676},
pages = {354},
title = {{Mastering the game of Go without human knowledge}},
volume = {550},
year = {2016}
}
@article{Vinyals2019,
abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8{\%} of officially ranked human players.},
author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"{e}}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'{e}}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"{u}}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
doi = {10.1038/s41586-019-1724-z},
file = {:home/malte/Documents/Uni/Semester4/NN/Seminar/NNSeminar{\_}OODP/docs/paper/Alphastar.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7782},
pages = {350--354},
pmid = {31666705},
publisher = {Springer US},
title = {{Grandmaster level in StarCraft II using multi-agent reinforcement learning}},
url = {http://dx.doi.org/10.1038/s41586-019-1724-z},
volume = {575},
year = {2019}
}
@article{Jaderberg2015,
abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner.In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
archivePrefix = {arXiv},
arxivId = {1506.02025},
author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
eprint = {1506.02025},
file = {:home/malte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaderberg et al. - 2015 - Spatial transformer networks(2).pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {2017--2025},
title = {{Spatial transformer networks}},
volume = {2015-Janua},
year = {2015}
}
@article{Zhu2018,
abstract = {Generalization has been one of the major challenges for learning dynamics models in model-based reinforcement learning. However, previous work on action-conditioned dynamics prediction focuses on learning the pixel-level motion and thus does not generalize well to novel environments with different object layouts. In this paper, we present a novel object-oriented framework, called object-oriented dynamics predictor (OODP), which decomposes the environment into objects and predicts the dynamics of objects conditioned on both actions and object-to-object relations. It is an end-to-end neural network and can be trained in an unsupervised manner. To enable the generalization ability of dynamics learning, we design a novel CNN-based relation mechanism that is class-specific (rather than object-specific) and exploits the locality principle. Empirical results show that OODP significantly outperforms previous methods in terms of generalization over novel environments with various object layouts. OODP is able to learn from very few environments and accurately predict dynamics in a large number of unseen environments. In addition, OODP learns semantically and visually interpretable dynamics models.},
archivePrefix = {arXiv},
arxivId = {1806.07371},
author = {Zhu, Guangxiang and Huang, Zhiao and Zhang, Chongjie},
eprint = {1806.07371},
file = {:home/malte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu, Huang, Zhang - 2018 - Object-oriented dynamics predictor(2).pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {9804--9815},
title = {{Object-oriented dynamics predictor}},
volume = {2018-Decem},
year = {2018}
}
